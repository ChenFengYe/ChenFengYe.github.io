<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<title>TightCap: 3D Human Shape Capture with Clothing Tightness Field, TOG 2021 </title>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

	<!-- Meta tags for Zotero grab citation -->
	<meta name="citation_title" content="TightCap: 3D Human Shape Capture with Clothing Tightness Field">
	<meta name="citation_author" content="Xin, Chen">
	<meta name="citation_author" content="Anpei, Pang">
	<meta name="citation_author" content="Wei, Yang">
	<meta name="citation_author" content="Peihao, Wang">
	<meta name="citation_author" content="Lan, Xu">
	<meta name="citation_author" content="Jingyi, Yu">
	<meta name="citation_publication_date" content="2021">
	<meta name="citation_conference_title" content="TOG">
	<meta name="citation_pdf_url" content="https://chenxin.tech/TightCap.html">

	<meta name="robots" content="index,follow">
	<meta name="description" content="In this paper, we present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan, which enables numerous applications such as virtual try-on, biometrics, and body evaluation. To break the severe variations of the human poses and garments, we propose to model the clothing tightness field – the displacements from the garments to the human shape implicitly in the global UV texturing domain. To this end, we utilize an enhanced statistical human template and an effective multi-stage alignment scheme to map the 3D scan into a hybrid 2D geometry image. Based on this 2D representation, we propose a novel framework to predict clothing tightness field via a novel tightness formulation, as well as an effective optimization scheme to further reconstruct multi-layer human shape and garments under various clothing categories and human postures. We further propose a new clothing tightness dataset (CTD) of human scans with a large variety of clothing styles, poses, and corresponding ground-truth human shapes to stimulate further research. Extensive experiments demonstrate the effectiveness of our TightCap to achieve the high-quality human shape and dressed garments reconstruction, as well as the further applications for clothing segmentation, retargeting, and animation.">
	<link rel="author" href="https://chenxin.tech/"/>

	<!-- Fonts and stuff -->
	<link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,800italic,400,700,800' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" type="text/css" href="css/project.css" media="screen" />
	<link rel="stylesheet" type="text/css" media="screen" href="css/iconize.css" />
	<script src="js/google-code-prettify/prettify.js"></script>
</head>

<body>
	<div id="content">
		<div id="content-inner">
			<div class="section logos"  style="text-align:center">
				<a href="http://www.shanghaitech.edu.cn/" target="_blank"><img src="images/shanghaitech-logo.svg" height="60"></a>
				<a href="http://vic.shanghaitech.edu.cn/" target="_blank"><img src="images/center-logo.png" height="60"></a>
				<a href="https://www.ucas.ac.cn/" target="_blank"><img src="images/UCAS.jpg" height="60"></a>
			</div>

			<div class="section head">
			
				<h1>TightCap: 3D Human Shape Capture <br>with Clothing Tightness Field</h1>
				<!-- <p><img alt="figure" src="./ProjectPage/image/AutoSweep_tesar.jpg" id = "Tesar"/></p> -->

				<div class="authors">
					<a href="http://chenxin.tech/" target="_blank">Xin Chen</a><sup>1,2</sup>&#160;&#160;					
					Anqi Pang<sup>1,2</sup>&#160;&#160;
					<a href="https://scholar.google.com/citations?user=fRjxdPgAAAAJ&hl=en" target="_blank">Wei Yang</a><sup>3</sup>&#160;&#160;
					Peihao Wang<sup>1,2</sup>&#160;&#160;
					<a href="http://xu-lan.com/" target="_blank">Lan Xu</a><sup>1</sup>&#160;&#160;
					<a href="http://vic.shanghaitech.edu.cn/vrvc/en/people/jingyi-yu/" target="_blank">Jingyi Yu</a><sup>1</sup>&#160;&#160;
				</div>

				<div class="affiliations">
					<sup>1</sup><a href="http://www.shanghaitech.edu.cn/" target="_blank">ShanghaiTech University</a> &#160;&#160;
					<sup>2</sup><a href="https://www.leeds.ac.uk/" target="_blank">University of Chinese Academy of Sciences</a>&#160;&#160;
					<sup>3</sup><a href="https://www.hust.edu.cn/" target="_blank">Huazhong University of Science and Technology</a>&#160;&#160;
				</div>

				<div class="venue">(<a href="http://cvpr2021.thecvf.com/" target="_blank">TOG 2021 - ACM Transactions on Graphics</a>)</div>
				<!-- <div class="venue">(Special issue in human pose, motion, activities, and shape in 3d)</div> -->
			</div>
			<div class="section teaser">
				<!-- <iframe width="640" height="360" src="data/video.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
				<!-- <iframe src="https://www.youtube.com/embed/HCC0z1WkQGc" allow="autoplay; encrypted-media" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen="" width="560" height="315" frameborder="0"></iframe> -->
				<!-- <iframe src="data/video.mp4" allow="autoplay; encrypted-media" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen="" width="560" height="315" frameborder="0"></iframe> -->
				<iframe src="data/video.mp4" allow="autoplay; encrypted-media" preload="" muted="" allowfullscreen="" width="600" height="352" frameborder="0"></iframe> 
				<!-- <video width="80%" playsinline="" playbackRate="5.0"  autoplay="autoplay" loop="loop" preload="" muted="" width="560" height="315"> -->
					<!-- <source src="data/video.mp4" type="video/mp4"> -->
				<!-- </video> -->
				<p style="font-size:11px; text-align:center">
					Download Video: <a href="data/video.mp4" target="_blank">HD</a> (MP4, 59 MB)
				</p>
			</div>

			<div class="section abstract">
				<h2>Abstract</h2>
				<p>
					In this paper, we present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan, which enables numerous applications such as virtual try-on, biometrics, and body evaluation. To break the severe variations of the human poses and garments, we propose to model the clothing tightness field – the displacements from the garments to the human shape implicitly in the global UV texturing domain. To this end, we utilize an enhanced statistical human template and an effective multi-stage alignment scheme to map the 3D scan into a hybrid 2D geometry image. Based on this 2D representation, we propose a novel framework to predict clothing tightness field via a novel tightness formulation, as well as an effective optimization scheme to further reconstruct multi-layer human shape and garments under various clothing categories and human postures. We further propose a new clothing tightness dataset (CTD) of human scans with a large variety of clothing styles, poses, and corresponding ground-truth human shapes to stimulate further research. Extensive experiments demonstrate the effectiveness of our TightCap to achieve the high-quality human shape and dressed garments reconstruction, as well as the further applications for clothing segmentation, retargeting, and animation.
				</p>
			</div>

			<div class="section downloads">
				<h2>Downloads</h2>
				<center>
				<ul>
					<li class="grid">
						<div class = "griditem">
							<a href="data/TightCap.pdf" target="_blank" class="imageLink"><img src = "images/pdf.png"></a><br/>
							Paper<br/>
							<a href="data/TightCap.pdf" target="_blank">PDF, 39 MB</a>
						</div>
					</li>
					<li class="grid"> 
						<div class = "griditem"> 
						<a href="data/video.mp4" target="_blank" class="imageLink"><img src = "images/mp4.png"></a><br />
						Video<br /> 
						<a href="data/video.mp4" target="_blank">MP4, 59 MB</a>
						<br/> 
						</div>
					</li>
					<li class="grid"> 
						<div class = "griditem"> 
						<a href="https://github.com/ChenFengYe/TightCap" target="_blank" class="imageLink"><img src = "images/dataset.png"></a><br />
						Dataset<br /> 
						<a href="https://github.com/ChenFengYe/TightCap" target="_blank">Coming Soon</a>
						<!-- <a href="https://drive.google.com/file/d/1O62Tp-2pPncuKD8oKbkP_WZJcbj-f_ik/view?usp=sharing" target="_blank">Dataset, 2.8 GB</a>						 -->
						<br/> 
						</div>
					</li>
					<li class="grid"> 
						<div class = "griditem"> 
						<a href="https://github.com/ChenFengYe/TightCap" target="_blank" class="imageLink"><img src = "images/github.png"></a><br />
						Code<br />
						<a href="https://github.com/ChenFengYe/TightCap" target="_blank">GitHub</a>
						<!-- <a href="https://github.com/ChenFengYe/AutoSweep" target="_blank">GitHub</a> -->
						<br/> 
						</div>
					</li>					
				</ul>
				</center>
			</div>
			<br />
			<div class="section abstract">
				<h2>Dataset</h2>
				<center>
					<video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
						<source src="data/video_dataset.mp4" type="video/mp4">
					</video>
				</center>
			</div>
			<div class="section abstract">
				<h2>Results of clothing retargeting and animation</h2>
				<center>
					<video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
						<source src="data/video_results_simulation.mp4" type="video/mp4">
					</video>
				</center>
			</div>
			<div class="section abstract">
				<h2>Results on 3D human clothing and body shape capture</h2>
				<center>
					<video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
						<source src="data/video_results_capture.mp4" type="video/mp4">
					</video>
				</center>
			</div>
			<div class="section list">
				<h2>Citation</h2>
				<p><a href="data/bibtex.bib" target="_blank">BibTeX, 1 KB</a></p>
				<div class="section bibtex">
					<pre>
@article{chen2021tightcap,
  title={TightCap: 3D Human Shape Capture with Clothing Tightness Field},
  author={Chen, Xin and Pang, Anqi and Wei, Yang and Peihao, Wang and Xu, Lan and Yu, Jingyi},
  journal={ACM Transactions on Graphics},
  year={2021}
}
</div>
			</div>
<!-- 			<div class="section acknowledgments">
				<h2>Acknowledgments</h2>
				<p>
					This work was funded by the ERC Consolidator Grant 4DRepLy (770784).
				</p>
			</div> -->

			<div class="section contact">
				<h2>Contact</h2>
				If you have any question, please feel free to ask
				Xin Chen, <a href='mailt&#111;&#58;chenxin2&#64;shanghaitech&#46;edu&#46;cn'>chenxin2&#64;shanghaitech&#46;edu&#46;cn.</a><br />
			</div>

			<!--<div class="section">
				<hr class="smooth">
				This page is <a href="http://www.zotero.org" target="_blank">Zotero</a> translator friendly. Page last updated 
				<script type="text/javascript">
					var m = "This page was last updated: " + document.lastModified;
					var p = m.length-9;
					document.writeln("<left>");
					document.write(m.substring(p, 0) + ".");
					document.writeln("</left>");
				</script>
				<a href="https://www.mpi-inf.mpg.de/imprint/">Imprint</a>. <a href="https://data-protection.mpi-klsb.mpg.de/inf/gvv.mpi-inf.mpg.de/projects/">Data Protection</a>.
			</div> -->
		</div>
	</div>
</body>
</html>
