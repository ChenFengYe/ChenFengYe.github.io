<html lang="cx">
<head>
<title>TightCap ProjectPage</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta charset="utf-8" />
<meta name="keywords" content="" />
<script>
	addEventListener("load", function () {
		setTimeout(hideURLbar, 0);
	}, false);

	function hideURLbar() {
		window.scrollTo(0, 1);
	}
</script>
<!-- Custom Theme files -->
<link href="css/style.css" type="text/css" rel="stylesheet" media="all">

</head>
<body>
<dir class = "WrapperBody">
<h1 id = "title"><center>TightCap: 3D Human Shape Capture<br>with Clothing Tightness</h1>
<h3><center><strong><a href="http://chenxin.tech">Xin Chen<sup>1</sup></a></strong>, &nbspAnqi Pang<sup>1</sup>, &nbsp<a href="https://scholar.google.com/citations?user=fRjxdPgAAAAJ&hl=en">Wei Yang<sup>2</sup></a>, &nbsp<a href="https://www.xu-lan.com/">Lan Xu<sup>2,3</sup></a>, &nbsp<a href="http://vic.shanghaitech.edu.cn/vrvc/en/people/jingyi-yu/">Jingyi Yu<sup>1</sup></a></h3>
<h3><center>
  <a href="http://www.shanghaitech.edu.cn/"><sup>1</sup>ShanghaiTech University</a>, &nbsp; <a href="https://www.dgene.com/cn/"><sup>2</sup>DGene</a>, &nbsp; <a href="http://www.ust.hk/"><sup>3</sup>Hong Kong University of Science and Technology</a>
</center></h3>
	
<p><img alt="figure" src="./image/TightCap_tesar.jpg" id = "Tesar"/></p>
<h1 id = "title1">Abstract</h1>
<hr>
<p>We present a learning-based scheme for estimating clothing tightness as well as the human shape on clothed 3D human scans robustly and accurately. Our approach maps the clothed human geometry/appearance to a geometry image that we call clothed-GI. To align clothed-GI under different clothing, we extend the parametric human model and employ skeleton detection and warping for reliable alignment. For each pixel on the clothed-GI, we extract a feature vector including color/texture, position, normal, etc. and train a modified conditional GAN network for per-pixel tightness prediction using a comprehensive 3D clothing. Our technique significantly improves the accuracy of human shape prediction, especially under loose and fitted clothing. We further demonstrate using our results for human/clothing segmentation, cloth retargeting and animations.</p>
<br>
<p>[<a href="https://arxiv.org/abs/1904.02601">ArxivPage</a>] [<a href="./TightCap_arxiv2020_paper.pdf">Paper</a>] [<a href="https://github.com/ChenFengYe/TightCap">Code</a>] [<a href="./TightCap_arxiv2020_video.mp4">Video</a>] [BibTex] </p>
<h1 id = "title1">Pipeline</h1>
<hr>
<p><img alt="figure" src="./image/TightCap_whole_pipeline.jpg"/>
<br>
The pipeline of TightCap. The first step is to warp our enhanced clothing-adapted SMPL with scanned mesh. Then, we deform warped mesh using Multi-Stage Alignment. After, we estimate the tightness map and clothing mask from mapped clothed-GI with Tightness Prediction. The final step, Multi-layer Reconstruction is to recover body shape from predicted tightness on the mesh, and segment cloth.</p>
<!-- br><br><br>
<p><img alt="figure" src="./image/TightCap_network_pipeline.jpg"/>
The network structure. The structure of our GeoNet is composed by an instance segmentation network (Mask R-CNN) and a deformable convolutional network. The net outputs instance masks labeled as semantic parts (profiles, bodies).</p-->
<h1 id = "title1">CTD Dataset</h1>
<hr>
<h4>Download</h4>

<p>The Clothing Tightness Dataset(CTD) is coming soon. Before this, you can download our sampling dataset with <a href="https://drive.google.com/file/d/1gihgQLVnfWCyL4cDXKcWpV1uSX_Lv1bY/view?usp=sharing"><U>this Google Drive link</U></a>. </p>

<h4>Introduce our dataset</h4>

<!--h4>Part 1: Image</h4>

<p>This folder in our dataset is including 11657 images with cubes and cylinders. The real dataset contains about 6000 unannotated images from <a href="http://www.image-net.org/">ImageNet</a>, 774 annotated images from <a href="http://3dvision.princeton.edu/projects/2012/SUNprimitive/">Xiao et al.</a>, and 4883 images collected from the Internet.</p>

<h4>Part 2: Annotation</h4>

<p>Annotation of each image by segmentation
label methods:
We use color to encode the instance and label information.</p>

<ul><li>Red channel: {10,20,...} represents {instance 1, instance 2,...}.</li><li>Blue channel: zero represents body, nonzero represents top face.</li><li>Red channel: 150 represents grip, 200 represents cylinder, 255 represents cube.</li></ul>

<p>Example is like below:</p>

<table border="1" cellspacing="0" cellpadding="0" align = "center" text-align = "center">
    <tr>
      <td align=center>Label</td>
      <td align=center>Color</td>
      <td align=center>instance ID</td>
    </tr>
    <tr>
      <td>Cylinder - top face</td>
      <td align=center>(10, 10, 200)</td>
      <td align=center>1</td>
    </tr>
    <tr>
      <td>Cylinder - top face</td>
      <td align=center>(20, 20, 200)</td>
      <td align=center>2</td>
    </tr>
    <tr>
      <td>Cylinder - body</td>
      <td align=center>(10, 0, 200)</td>
      <td align=center>1</td>
    </tr>
    <tr>
      <td>Cube - top face</td>
      <td align=center>(10, 10, 255)</td>
      <td align=center>1</td>
    </tr>
    <tr>
      <td>Cube - body</td>
      <td align=center>(10, 0, 255)</td>
      <td align=center>1</td>
    </tr>
    <tr>
      <td>Grip</td>
      <td align=center>(10, 0, 150)</td>
      <td align=center>1</td>
    </tr>
</table>

<h4>Part 3: ImageSets</h4>

<p>This is further separated into 8183 training images and 3474 testing images.</p>
<h1 id = "title1">Code</h1>
<hr>
<p>You can visit the <a href="https://github.com/ChenFengYe/TightCap"><U>GithubPage</U></a> for code.
<br>
<br>
<p>The code consists of two modules, as mentioned in our paper, the learning module (image to mask) and the graphics module (mask to 3d mesh). The first module follows the framework of FCIS and Mask RCNN. A common learning framework with Python. The second module is built based on Unity3D and our own framework. The purpose of the whole module is to sweep the profiles with a dynamic demo. I need to remind you that the code is not convenient for a beginner on 3D. It is really hard to configure this module, I sincerely suggest you refer to this code only.
<br>
<br>
<p>The code might no longer be maintained. However, I still hope part of our work can give you help or inspiration. If you have any questions, please feel free to ask <a href="http://chenxin.tech"> me</a>.
<br>
<br>
<p>Code scripts for second module:
<p>TightCap_ObjectSnapping/Assets/BodyEngine.cs
<p>TightCap_ObjectSnapping/Assets/FaceEngine.cs
<p>TightCap_ObjectSnapping/Assets/GraphicsEngine.cs</p-->

<h1 id = "title1">Results</h1>
<hr>
<p><img alt="figure" src="./image/TightCap_results.jpg"/></p>
<h1 id = "title1">Citation</h1>
<hr>
<p>Please cite this paper in your publications if it helps your research:</p>
<blockquote><p>https://arxiv.org/abs/1904.02601</p></blockquote>

<h1 id = "title1">License</h1>
<hr>
TightCap dataset is freely available for free non-commercial use. <br>For commercial purpose, please contact to <a href="http://chenxin.tech"> Xin Chen</a> or <a href="http://vic.shanghaitech.edu.cn/vrvc/en/people/jingyi-yu/">Jingyi Yu</a>.
</dir>
</body>
</html>
