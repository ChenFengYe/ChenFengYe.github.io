<!DOCTYPE html>
<html lang="cx">

<head>
  <title>Xin Chen's Homepage - Shanghaitech University - UCAS</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8" />
  <meta name="keywords" content="" />
  <script>
    addEventListener("load", function () {
      setTimeout(hideURLbar, 0);
    }, false);

    function hideURLbar() {
      window.scrollTo(0, 1);
    }
  </script>
  <!-- Custom Theme files -->
  <link href="css/bootstrap.css" type="text/css" rel="stylesheet" media="all">
  <link href="css/style.css" type="text/css" rel="stylesheet" media="all">
  <!-- font-awesome icons -->
  <link href="css/fontawesome-all.min.css" rel="stylesheet">
  <!-- //Custom Theme files -->
  <!-- online-fonts -->
  <link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900" rel="stylesheet">
  <!-- //online-fonts -->
</head>

<body>
  <div class="sidenav">
    <div class="side_top"> <img src="images/chenxin.jpg" alt="news image" class="img-fluid navimg">
      <h1> Xin Chen - 陈欣 <br>
        <!--[CV - <a href="files/CV/chenxin_cv_cn_201803.pdf"><font size="4">中文</font></a> / <a href="files/CV/chenxin_cv_en_202007.pdf">English</a>]<br>-->
        <!--[CV - <font size="4">中文</font>/ <a href="files/CV/chenxin_cv_en_202011.pdf">English</a>]<br>-->
        <a href="files/CV/chenxin_cv_en_202108.pdf">CV | Resume</a> - 简历<br>
        <a href="mailto:chenxin2@shanghaitech.edu.cn">Email</a> | <a
          href="https://www.linkedin.com/in/xin-chen-cs/">LinkedIn</a> | <a
          href="https://github.com/ChenFengYe">GitHub</a><br>
        <a href="https://aideadlin.es/?sub=ML,CV,CG,NLP,RO,SP,DM">Conference Timer</a><br>
        <a href="https://scholar.google.com/citations?user=7qeAJZ4AAAAJ&hl=en">Google Scholar</a>
      </h1>
    </div>
    <!-- header -->
    <header>
      <div class="container-fluid px-5 ">
        <nav class="mnu mx-auto">
          <label for="drop" class="toggle">Menu</label>
          <input type="checkbox" id="drop">
          <ul class="menu">
            <!--<li><a href="index.html">Home</a></li-->
            <li class="mt-sm-0"><a href="#about" class="scroll">Bio</a></li>
            <!-- <li class="mt-sm-3"><a href="./Art.html" class="scroll">Art</a></li> -->
            <li class="mt-sm-3"><a href="#publication" class="scroll">Publications</a></li>
            <li class="mt-sm-3"><a href="#experience" class="scroll">Experience</a></li>
            <!-- <li class="mt-sm-3"><a href="#skill" class="scroll">Skills</a></li> -->
          </ul>
        </nav>
      </div>
    </header>
  </div>
  <div class="main" id="about">
    <div class="banner-text-w3ls">
      <div>
        <div class="mx-auto text-left">
          <h1><strong>Xin Chen - 陈 欣</strong></h1>
          <h5>Researcher, &ensp;Ph.D.</h5>
          <h6>chenxin2@shanghaitech.edu.cn</h6><br>
          <h5><a href="https://www.tencent.com/"><b>Tencent - 腾讯</b></a></h5>
          <h5><a href="http://english.ucas.ac.cn/"><b>University of Chinese Academy of Sciences - 中国科学院大学 </b></a></h5>
          <h5><a href="http://www.shanghaitech.edu.cn/"><b>ShanghaiTech University - 上海科技大学 </b></a> - <a
              href="http://sist.shanghaitech.edu.cn/">SIST</a> - <a href="http://vic.shanghaitech.edu.cn/">VDI</a></h5>
          <!-- <h6>&emsp;<a href="http://sist.shanghaitech.edu.cn/">School of Information Science and Technology</a></h6> -->
          <!-- <br> -->
          <!-- <h5><a href="https://www.dgene.com/cn/">DGene</a> </h5> -->
          <!-- <p>. </p> -->
          <!-- <p class="banp mt-5"> </p> -->
          <br>
          Hi, I am a researcher at <a href="https://www.tencent.com/">Tencent</a>, led by <a
            href="https://www.skicyyu.org/">Gang Yu</a>, working on <strong>Generative Artificial Intelligence</strong>.
          <!-- and <a href="https://cshen.github.io/">Chunhua Shen</a> -->
          <!-- <br><br> -->
          I received my Ph.D. degree from <a href="http://www.shanghaitech.edu.cn/">ShanghaiTech University</a> and <a
            href="http://english.ucas.ac.cn/">University of Chinese Academy of Sciences</a>, advised by <a
            href="http://www.yu-jingyi.com/">Prof. Jingyi Yu</a>. Before this, I was supervised by <a
            href="http://youyizheng.net/">Prof. Youyi Zheng</a> for two years, working on image-based modeling.
          <br><br>
          My research interests are <strong>Computer Vision</strong> for <strong>Graphics</strong>, especially:
          </p>
          <li>3D Generation,&nbsp;&nbsp;Avatar Generation,&nbsp;&nbsp;Motion Synthesis,&nbsp;&nbsp;View Synthesis</li>
          <li>Human Performance Capture,&nbsp;&nbsp;Motion Capture,&nbsp;&nbsp;Deep Learning</li>
          <li>Image-based Modeling,&nbsp;&nbsp;Neural Rendering,&nbsp;&nbsp;3D Human Modeling and Dynamics</li>
          <!-- <li>Geometric deep learning</li>        -->
          <!--a class="btn btn-primary mt-lg-5 mt-3 agile-link-bnr" href="#experience" role="button">Learn More</a-->
          <br>
          <font color="#f33"><b>News!</b></font>
          </p>
          <!-- <li>[2022.11]&ensp; Submit four4 Papers.</li> -->
          <li>[2023.02]&ensp; 2 papers accepted at <font color="#f33"><b>CVPR 2023</b></font>.</li>
          <li>[2022.02]&ensp; Join Tencent as a Research Scientist.</li>
          <li>[2021.11]&ensp; Receive my Ph.D. degree.</li>
          <li>[2021.10]&ensp; TightCap will be presented at ACM <font color="#f33"><b>SIGGRAPH 2022</b></font>.</li>
          <!-- <li>[2021.07]&ensp; TightCap is accepted by TOG. Dataset is released.</li>
          <li>[2021.06]&ensp; SportsCap is accepted by IJCV. Dataset is released.</li> -->
        </div>
      </div>
    </div>
    <section class="publication" id="publication">
      <div class="container-fluid py-lg-1">
        <h3 class="w3_head mb-5">Selected Publications <font size="5"><a href="./Publication.html">(Complete List…)</a>
          </font>
        </h3>
        <!-- <h3 class="w3_head mb-5">Selected Publications (<a href="./Publication.html">Complete List…</a>)</h3> -->
        <div class="col-lg-12">
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <img src="files/Paper/CVPR23_MLD/teaser.png" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
            </div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4>Executing your Commands via Motion Diffusion in Latent Space<br>
              </h4>
              <p><strong>Xin Chen</strong>, Biao Jiang, <a
                  href="https://scholar.google.com/citations?user=A6K6bkoAAAAJ&hl=en">Wen Liu</a>, <a
                  href="https://speedinghzl.github.io/">Zilong Huang</a>, Bin Fu, <a
                  href="https://faculty.fudan.edu.cn/chentao1/zh_CN/">Tao Chen</a>, <a
                  href="http://www.yu-jingyi.com/">Jingyi Yu</a>, <a href="https://www.skicyyu.org/">Gang Yu</a><br>
              </p>
              <p> <font color="#000000"><b>CVPR 2023</b></font></p>
              <!-- <p>(<strong>SIGGRAPH 2022</strong>) ACM Transactions on Graphics, TOG Journal Track</p> -->
              <!-- <p class="text-left">We present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan. </p> -->
              <p>[<a href="https://chenxin.tech/mld">Project</a>]
                [<a href="https://arxiv.org/abs/2212.04048" target=_blank>Paper</a>]
                [<a href="https://github.com/ChenFengYe/motion-latent-diffusion" target=_blank>Code</a>]
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <img src="files/Paper/CVPR23_DETR/teaser.png" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
            </div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4>End-to-End 3D Dense Captioning with Vote2Cap-DETR<br>
              </h4>
              <p>Sijin Chen, <a href="https://hongyuanzhu.github.io/">Hongyuan Zhu</a> <strong>Xin Chen</strong>, Yinjie Lei, <a
                  href="https://faculty.fudan.edu.cn/chentao1/zh_CN/">Tao Chen</a>, <a href="https://www.skicyyu.org/">Gang Yu</a><br>
              </p>
              <p> <font color="#000000"><b>CVPR 2023</b></font></p>
              <!-- <p>(<strong>SIGGRAPH 2022</strong>) ACM Transactions on Graphics, TOG Journal Track</p> -->
              <!-- <p class="text-left">We present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan. </p> -->
              <p>[<a href="https://arxiv.org/abs/2301.02508" target=_blank>Paper</a>]
                [<a href="https://github.com/ch3cook-fdu/Vote2Cap-DETR" target=_blank>Code</a>]
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <video
                src="files/Paper/TOG2021_TightCap/project_page_TightCap/data/video_teaser.mp4" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
            </div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4><strong>TightCap</strong>: 3D Human Shape Capture with Clothing Tightness Field<br>
              </h4>
              <p><strong>Xin Chen</strong>, Anqi Pang, <a
                  href="https://scholar.google.com/citations?user=fRjxdPgAAAAJ&hl=en">Wei Yang</a>, Peihao Wang, <a
                  href="https://www.xu-lan.com/">Lan Xu</a>, <a href="http://www.yu-jingyi.com/">Jingyi Yu</a><br>
              </p>
              <p> <font color="#000000"><b>SIGGRAPH 2022</b></font> TOG Journal Track</p>
              <!-- <p>(<strong>SIGGRAPH 2022</strong>) ACM Transactions on Graphics, TOG Journal Track</p> -->
              <!-- <p class="text-left">We present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan. </p> -->
              <p>[<a href="./TightCap">Project</a>] [<a href="https://github.com/ChenFengYe/TightCap">Code</a>] [<a
                  href="https://arxiv.org/abs/1904.02601">Arxiv</a>]
                [<a href="files/Paper\TOG2021_TightCap\project_page_TightCap\data\TightCap.pdf" target=_blank>Paper</a>]
                [<a href="files/Paper\TOG2021_TightCap\project_page_TightCap\data\video.mp4" target=_blank>Video</a>]
                [<a href="files\Paper\TOG2021_TightCap\project_page_TightCap\data\bibtex.html" target=_blank>BibTex</a>]
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <video
                src="files/Paper/IJCV2020_Sport/project_page_SportsCap/data/video_teaser.mp4" width="90%" playsinline=""
                autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid"></div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4><strong>SportsCap</strong>: Monocular 3D Human Motion Capture and Fine-grained Understanding in
                Challenging Sports Videos<br>
              </h4>
              <p><strong>Xin Chen</strong>, Anqi Pang, <a
                  href="https://scholar.google.com/citations?user=fRjxdPgAAAAJ&hl=en">Wei Yang</a>, <a
                  href="http://yuexinma.me/aboutme.html">Yuexin Ma</a>, <a href="http://xu-lan.com/">Lan Xu</a>, <a
                  href="http://www.yu-jingyi.com/">Jingyi Yu</a><br>
              </p>
              <p><font color="#000000"><b>IJCV 2021</b></font></p>
              <!-- <p>(<strong>IJCV 2021</strong>) International Journal of Computer Vision</p> -->
              <!-- <p class="text-left">We propose SportsCap – the first approach for simultaneously capturing 3D human motions and understanding fine-grained actions from monocular challenging sports video input. </p> -->
              <p>[<a href="./SportsCap.html">Project</a>] [<a href="https://github.com/ChenFengYe/SportsCap">Code</a>]
                [<a href="https://arxiv.org/abs/2104.11452">Arxiv</a>]
                [<a href="files\Paper\IJCV2020_Sport\project_page_SportsCap\data\SportsCap.pdf">Paper</a>] [<a
                  href="files\Paper\IJCV2020_Sport\project_page_SportsCap\data\video.mp4" target=_blank>Video</a>] [<a
                  href="files\Paper\IJCV2020_Sport\project_page_SportsCap\data\bibtex.html" target=_blank>BibTex</a>]
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <video src="files/Paper/CVPR2021_ChallenCap/video_teaser.mp4"
                width="90%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image"
                class="img-fluid" /></div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4><strong>ChallenCap</strong>: Monocular 3D Capture of Challenging Human Performances using Multi-Modal
                References<br>
              </h4>
              <p><a href="https://hynann.github.io/">Yannao He</a>, Anqi Pang, <strong>Xin Chen</strong>, Han Liang, <a
                  href="http://yuexinma.me/aboutme.html">Yuexin Ma</a>, <a href="http://xu-lan.com/">Lan Xu</a><br>
              </p>
              <p><font color="#000000"><b>CVPR 2021 Oral</b></font></p>
              <!-- <p>(<strong>CVPR 2021 Oral</strong>) Conference on Computer Vision and Pattern Recognition</p> -->
              <!-- <p class="text-left">We propose SportsCap – the first approach for simultaneously capturing 3D human motions and understanding fine-grained actions from monocular challenging sports video input. </p> -->
              <p>[Project] [<a href="https://arxiv.org/abs/2103.06747">Arxiv</a>] [<a
                  href="https://openaccess.thecvf.com/content/CVPR2021/papers/He_ChallenCap_Monocular_3D_Capture_of_Challenging_Human_Performances_Using_Multi-Modal_CVPR_2021_paper.pdf"
                  target=_blank>Paper</a>] [<a href="https://www.youtube.com/watch?v=ctF0xUMoN2E"
                  target=_blank>Video</a>] [<a href="files\Paper\CVPR2021_ChallenCap\bibtex.html"
                  target=_blank>BibTex</a>] </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img">
              <img src="files/Paper/AllPublications/AAAI2022.png" width="90%" playsinline="" autoplay="autoplay"
                loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid" />
            </div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4>Anisotropic Fourier Features for Neural Image-Based Rendering and Relighting</h4>
              <p>Huangjie Yu, Anpei Chen, <strong>Xin Chen</strong>, <a href="https://www.xu-lan.com/">Lan Xu</a>, Ziyu
                Shao, <a href="http://www.yu-jingyi.com/">Jingyi Yu</a></p>
              <p><font color="#000000"><b>AAAI 2022 Oral</b></font></p>
              <!-- <p>(<strong>AAAI 2022 Oral</strong>) the Association for the Advance of Artificial Intelligence</p> -->
              <!-- <p class="text-left">We present a fully automatic framework for extracting editable 3D objects directly from a single photograph. </p> -->
              <p>[<a href="https://www.aaai.org/AAAI22Papers/AAAI-2220.YuH.pdf">Paper</a>] [BibTex]
              </p>
            </div>
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <video src="files/Paper/IJCAI2021_FewShot/video_teaser.mp4"
                width="90%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image"
                class="img-fluid" /></div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4>Few-shot Neural Human Performance Rendering from Sparse RGBD Videos<br>
              </h4>
              <p>Anqi Pang*, <strong>Xin Chen*</strong>, Haimin Luo, <a href="https://wuminye.com/">Mingye Wu</a>, <a
                  href="http://www.yu-jingyi.com/">Jingyi Yu</a>, <a href="http://xu-lan.com/">Lan Xu</a><br>
              </p>
              <p><font color="#000000"><b>IJCAI 2021</b></font></p>
              <!-- <p>(<strong>IJCAI 2021</strong>) International Joint Conference on Artificial Intelligence</p> -->
              <!-- <p class="text-left">We propose SportsCap – the first approach for simultaneously capturing 3D human motions and understanding fine-grained actions from monocular challenging sports video input. </p> -->
              <p>[Project] [<a href="https://arxiv.org/abs/2107.06505">Arxiv</a>] [<a
                  href="https://arxiv.org/abs/2107.06505" target=_blank>Paper</a>] [<a
                  href="files\Paper\IJCAI2021_FewShot\video_teaser.mp4" target=_blank>Video</a>] [<a
                  href="files\Paper\IJCAI2021_FewShot\bibtex.html" target=_blank>BibTex</a>]
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <video src="files\Paper\MM2021_HOIFVV\video_teaser.mp4" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image"
                class="img-fluid" /></div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4>Neural Free-Viewpoint Performance Rendering under ComplexHuman-object Interactions<br>
              </h4>
              <p>Guoxing Sun, <strong>Xin Chen</strong>, Yizhang Chen, Anqi Pang, Pei Lin, Yuheng Jiang, <a
                  href="http://xu-lan.com/">Lan Xu</a>, <a
                  href="http://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a>, <a
                  href="http://www.yu-jingyi.com/">Jingyi Yu</a><br>
              </p>
              <p><font color="#000000"><b>ACMMM 2021</b></font></p>
              <!-- <p>(<strong>ACMMM 2021</strong>) ACM Multimedia</p> -->
              <!-- <p class="text-left">We propose SportsCap – the first approach for simultaneously capturing 3D human motions and understanding fine-grained actions from monocular challenging sports video input. </p> -->
              <p>[Project] [<a href="https://arxiv.org/abs/2108.00362">Arxiv</a>] [<a
                  href="https://arxiv.org/abs/2108.00362" target=_blank>Paper</a>] [<a
                  href="files\Paper\MM2021_HOIFVV\video_teaser.mp4" target=_blank>Video</a>] [<a
                  href="files\Paper\MM2021_HOIFVV\bibtex.html" target=_blank>BibTex</a>]
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <video src="files/Paper/CVPR2018_Face/3d_face.mp4" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
            </div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4>Sparse Photometric 3D Face Reconstruction Guided by Morphable Models</h4>
              <p> Xuan Cao, Zhang Chen, Anpei Chen, <strong>Xin Chen</strong>, Shiying Li, <a
                  href="http://www.yu-jingyi.com/">Jingyi Yu</a></p>
              <p><font color="#000000"><b>CVPR 2018</b></font></p>
              <!-- <p>(<strong>CVPR 2018</strong>) IEEE Conference on Computer Vision and Pattern Recognition</p> -->
              <!-- <p class="text-left">We present a novel 3D face reconstruction technique that leverages sparse photometric stereo (PS) and the latest advances on face registration modeling from a single image. </p> -->
              <p>[<a href="files/Paper/CVPR2018_Face/Sparse_Photometric_3D_CVPR_2018_paper.pdf">Paper</a>] [<a
                  href="files/Paper/CVPR2018_Face/Sparse_Photometric_3D_CVPR_2018_video.mp4">Video</a>] [<a
                  href="files/Paper/CVPR2018_Face/Sparse_Photometric_3D_CVPR_2018_supp.pdf">Supp</a>] [<a
                  href="files/Paper/CVPR2018_Face/cao2018sparse.html" target=_blank>BibTex</a>] </p>
            </div>
          </div>
          <div class="row paper_box">
            <!-- <div class="col-md-3 col-12 paper_img"> <img src="files/Paper/TVCG2018_AutoSweep/AutoSweep.jpg" alt="Popup Image" class="img-fluid" /></div> -->
            <div class="col-md-3 col-12 paper_img"> <video
                src="files/Paper/TVCG2018_AutoSweep/project_page_AutoSweep/data/video.mp4" width="90%" playsinline=""
                autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid"></div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4><strong>AutoSweep</strong>: Recovering 3D Editable Objects&nbsp;from a Single Photograph</h4>
              <p><strong>Xin Chen</strong>, <a href="http://liyuwei.cc/">Yuwei Li</a>, <a href="http://luoxi.tech/">Xi
                  Luo</a>, <a href="http://tianjiashao.com/">Tianjia Shao</a>, <a
                  href="http://www.yu-jingyi.com/">Jingyi Yu</a>, <a href="http://kunzhou.net/">Kun Zhou</a>, <a
                  href="http://youyizheng.net/">Youyi Zheng </a></p>
              <p><font color="#000000"><b>TVCG 2018</b></font></p>
              <!-- <p>(<strong>TVCG 2018</strong>) IEEE Transactions on Visualization and Computer Graphics</p> -->
              <!-- <p class="text-left">We present a fully automatic framework for extracting editable 3D objects directly from a single photograph. </p> -->
              <p>[<a href="./AutoSweep.html">Project</a>] [<a href="https://github.com/ChenFengYe/AutoSweep">Code</a>]
                [<a href="https://arxiv.org/abs/2005.13312">Arxiv</a>]
                [<a href="files/Paper/TVCG2018_AutoSweep/project_page_AutoSweep/data/AutoSweep.pdf">Paper</a>] [<a
                  href="files/Paper/TVCG2018_AutoSweep/project_page_AutoSweep/data/video.mp4">Video</a>] [<a
                  href="files/Paper/TVCG2018_AutoSweep/project_page_AutoSweep/data/bibtex.html"
                  target=_blank>BibTex</a>]
              </p>
              <br>
              <p>
              <h4><a href="./Publication.html">See all publications ...</a></h4>
              </p>
            </div>
            <div class="row paper_box">
              <!-- <div class="col-md-3 col-12 paper_img"> <img src="files/Paper/TVCG2018_AutoSweep/AutoSweep.jpg" alt="Popup Image" class="img-fluid" /></div> -->
              <div class="col-md-3 col-12 paper_img"></div>
              <!-- .Icon ends here -->
              <div class="col-md-9 col-12 paper_content">
                </p>
              </div>
              <!-- .Service-content ends here -->
            </div>
          </div>
        </div>
    </section>
    <!-- experience -->
    <section class="wedo" id="experience">
      <div class="experience">
        <h3 class="w3_head mb-1">Experience</h3>
        <div class="row service_w3top mt-5">
          <div class="col-xl-12">
            <div class="d-flex experience-box">
              <!-- <div class="popup"> <img src="images/g6.jpg" alt="Popup Image" class="img-fluid"></div> -->
              <div class="icon"> <span class="fa fa-briefcase"></span></div>
              <!-- <div class="icon"> <span class="fa fa-briefcase" style=" background-image: url('1.png'); background-repeat:no-repeat;display:block;align-items: center;" ></span> </div> -->
              <!-- .Icon ends here -->
              <div class="service-content">
                <div class="d-md-flex justify-content-between">
                  <h4><a href="https://open.youtu.qq.com/#/open">Tencent - 腾讯 </a></h4>
                </div>
                <div class="d-md-flex justify-content-between">
                  <h4> PCG QQ - Research Scientist</h4>
                  <h4> Feb. 2022 - Present</h4>
                </div>
                <div class="d-md-flex justify-content-between">
                  <h4> CSIG YouTu Lab -Research Scientist Intern</h4>
                  <h4>Nov. 2020 - Mar. 2021</h4>
                </div>
                <p>I have been working at Tencent, Shanghai.</p>
                <p>I work as a research scientist intern at <a href="https://open.youtu.qq.com/#/open">Tencent YouTu
                    Lab</a> in 2020 for researching on the 3D human body reconstruction.</p>
                <br>
              </div>
              <!-- .Service-content ends here -->
            </div>
            <div class="d-flex experience-box">
              <div class="icon"><span class="fa fa-briefcase"></span> </div>
              <!-- .Icon ends here -->
              <div class="service-content">
                <div class="d-md-flex justify-content-between">
                  <h4><a href="https://www.dgene.com/cn/">DGene Digital Technology Inc. - 上海叠境数字</a></h4>
                  <h4> Jul. 2018 - Dec. 2019</h4>
                </div>
                <h4> R&amp;D Intern</h4>
                <p>I work as a part-time research and development intern at <a href="https://www.dgene.com/eng/">DGene
                    Digital Technology Inc. </a>and won <strong>the Best Outstanding Intern Award</strong> in 2018 for
                  leading the mobile virtual fitting project.</p>
                <br>
              </div>
              <!-- .Service-content ends here -->
            </div>
          </div>
        </div>
        <div class="row service_w3top mt-5">
          <div class="col-xl-12">
            <div class="d-flex experience-box">
              <div class="icon"> <span class="fa fa-graduation-cap"></span> </div>
              <!-- .Icon ends here -->
              <div class="service-content">
                <div class="d-md-flex justify-content-between">
                  <h4><a href="http://english.ucas.ac.cn/">University of Chinese Academy of Sciences - 中国科学院大学</a></h4>
                  <h4>Sep. 2018 - Feb. 2022 </h4>
                </div>
                <h4> Ph.D. Degree</h4>
                <p>I received my Ph.D. degree from <a href="http://english.ucas.ac.cn/">UCAS</a> under
                  the supervision of <a href="http://www.yu-jingyi.com/">Prof. Jingyi Yu,</a> working on <strong>human
                    performance capture</strong> including human reconstruction, clothing capture, and shape estimation.
                </p>
                <br>
              </div>
              <!-- .Service-content ends here -->
            </div>
            <div class="d-flex experience-box">
              <div class="icon"> <span class="fa fa-graduation-cap"></span> </div>
              <!-- .Icon ends here -->
              <div class="service-content">
                <div class="d-md-flex justify-content-between">
                  <h4><a href="http://www.shanghaitech.edu.cn/eng/">ShanghaiTech University - 上海科技大学</a></h4>
                  <h4>Sep. 2016 - Jul. 2018 </h4>
                </div>
                <h4> Master Student</h4>
                <p>I spent 2 years on computer graphics research, advised by <a href="http://youyizheng.net/">Prof.
                    Youyi
                    Zheng</a> and developed a fully automatic framework for recovering 3D editable objects from a single
                  photograph, published on <strong>TVCG 2018</strong> (Top journal in graphics).</p>
                <br>
              </div>
              <!-- .Service-content ends here -->
            </div>
            <div class="d-flex experience-box">
              <div class="icon"> <span class="fa fa-graduation-cap"></span> </div>
              <!-- .Icon ends here -->
              <div class="service-content">
                <div class="d-md-flex justify-content-between">
                  <h4><a href="http://english.qut.edu.cn/">Qingdao University of Technology</a></h4>
                  <h4>Sep. 2012 - Jul. 2016 </h4>
                </div>
                <h4> Bachelor </h4>
                <p>I received a B.S. degree in electronic information science and technology and won honorable awards
                  including: </p>
                <li>National Encouragement Scholarship 2016</li>
                <li>Provincial Government Scholarship 2015</li>
                <li>Second Prize in China Mathematical Contest in Modeling 2015</li>
                <li>Province-level Merit Student 2013</li>
              </div>
              <!-- .Service-content ends here -->
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="wedo" id="-Supervisors">
      <h3 class="w3_head mb-1"> Supervisors </h3>
      <div id="id4" style="clear:both"></div>
      <div class="col-md-5 col-12 people-content">
        <p class="banp mt-5"> <a href="http://www.yu-jingyi.com/">Prof. Jingyi Yu</a> - Supervisor </p>
        <li>IEEE Fellow</li>
        <li>Program chair of ICPR 2020 and CVPR 2021</li>
        <li>Executive Dean, SIST, ShanghaiTech University</li>
        <li>Founder, DGene Inc.</li>
        <li>Ph.D.'s degree, MIT, 2005</li>
      </div>
      <div class="col-md-5 col-12 people-content">
        <p class="banp mt-5"> <a href="http://youyizheng.net/">Prof. Youyi Zheng</a> - Former Supervisor </p>
        <li>Hundred Talents Program, ZheJiang Univeristy</li>
        <li>Researcher, State Key Lab of CAD&CG, ZheJiang Univeristy</li>
        <li>Ph.D.'s degree, Hong Kong University of Science and Technology, 2011</li>
      </div>
      <div id="id4" style="clear:both"></div>
    </section>

    <!-- //skill -->
    <!-- <section class="wedo" id="skill">
      <h3 class="w3_head mb-1">Skills </h3>
      <p class="banp mt-5"> Programming Languages</p>
      <ul class="fa-ul mb-0">
        <li> <i class="fa-li fa fa-check"></i> C++ (OpenGL, OpenCV, Qt, Eigen, PCL, CUDA and so on. )</li>
        <li> <i class="fa-li fa fa-check"></i> C#</li>
        <li> <i class="fa-li fa fa-check"></i> Python (Pytorch, Tensorflow, Mxnet)</li>
      </ul>
      <p class="banp mt-5"> Softwares</p>
      <ul class="fa-ul mb-0">
        <li> <i class="fa-li fa fa-check"></i> Visual Studio, Pycharm, Jupyter Notebook, Android Studio</li>
        <li> <i class="fa-li fa fa-check"></i> Matlab</li>
        <li> <i class="fa-li fa fa-check"></i> Unity3D, Blender</li>
        <li> <i class="fa-li fa fa-check"></i> Adobe Photoshop, Premiere</li>
      </ul>
      <p class="banp mt-5"> Others</p>
      <ul class="fa-ul mb-0">
        <li> <i class="fa-li fa fa-check"></i> Latex, Markdown</li>
        <li> <i class="fa-li fa fa-check"></i> Leap Motion, HTC Vive</li>
      </ul>
      <!-- <script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.?i=5ix9r8rqpnb&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script> -->
      <!-- <script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=59qpe4r4c9p&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script> -->
    </section> -->
    <!-- //skill -->
    <!-- <section> -->
    <script type='text/javascript' id='clustrmaps'
      src='//cdn.clustrmaps.com/map_v2.js?cl=b5b5b5&w=300&t=tt&d=O8RXzIRuaS2aZrhlSVF9Mo3sC3cJziACrnPJbcG9mF4&co=ffffff&ct=000000&cmo=ff0000&cmn=00b6ff'></script>
</body>

</html>