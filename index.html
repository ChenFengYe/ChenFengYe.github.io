<!DOCTYPE html>
<html lang="cx">

<head>
  <title>Xin Chen's Homepage - Shanghaitech University - UCAS</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8" />
  <meta name="keywords" content="" />
  <script>
    addEventListener("load", function () {
      setTimeout(hideURLbar, 0);
    }, false);

    function hideURLbar() {
      window.scrollTo(0, 1);
    }
  </script>
  <!-- Custom Theme files -->
  <link href="css/layout.min.css" rel="stylesheet" type="text/css" />

  <link href="css/bootstrap.css" type="text/css" rel="stylesheet" media="all">
  <link href="css/style.css" type="text/css" rel="stylesheet" media="all">
  <!-- font-awesome icons -->
  <link href="css/fontawesome-all.min.css" rel="stylesheet">
  <!-- //Custom Theme files -->
  <!-- online-fonts -->
  <link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900" rel="stylesheet">
  <!-- <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet" type="text/css"/> -->
  <!-- //online-fonts -->

  <!-- PAGE LEVEL PLUGIN STYLES -->
  <link href="css/animate.css" rel="stylesheet">
  <link href="vendor/swiper/css/swiper.min.css" rel="stylesheet" type="text/css" />

  <!-- THEME STYLES -->

  <!-- FONT STYLES -->
  <!-- <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans"> -->

  <!-- Favicon -->
  <!-- <link rel="shortcut icon" href="favicon.ico"/> -->

</head>

<body>
  <div class="sidenav">
    <div class="side_top"> <img class="img-fluid navimg" src="images/chenxin.jpg" width="70%" height="70%"
        onmouseover="this.src='images/chenxin4.jpg'" onmouseout="this.src='images/chenxin.jpg'">
      <h1> Xin Chen - 陈欣 <br>
        <!--[CV - <a href="files/CV/chenxin_cv_cn_201803.pdf"><font size="4">中文</font></a> / <a href="files/CV/chenxin_cv_en_202007.pdf">English</a>]<br>-->
        <!--[CV - <font size="4">中文</font>/ <a href="files/CV/chenxin_cv_en_202011.pdf">English</a>]<br>-->
        <a href="files/CV/chenxin_cv_en_202311.pdf">CV | Resume</a> - 简历<br>
        <a href="mailto:chenxin2@shanghaitech.edu.cn">Email</a> | <a
          href="https://www.linkedin.com/in/xin-chen-cs/">LinkedIn</a> | <a
          href="https://github.com/ChenFengYe">GitHub</a><br>
        <a href="https://aideadlin.es/?sub=ML,CV,CG,NLP,RO,SP,DM">Conference Timer</a><br>
        <a href="https://scholar.google.com/citations?user=7qeAJZ4AAAAJ&hl=en">Google Scholar</a>
      </h1>
    </div>
    <!-- header -->
    <header>
      <div class="container-fluid px-5 ">
        <nav class="mnu mx-auto">
          <label for="drop" class="toggle">Menu</label>
          <input type="checkbox" id="drop">
          <ul class="menu">
            <!--<li><a href="index.html">Home</a></li-->
            <li class="mt-sm-0"><a href="#about" class="scroll">Bio</a></li>
            <!-- <li class="mt-sm-3"><a href="./Art.html" class="scroll">Art</a></li> -->
            <li class="mt-sm-3"><a href="#services" class="scroll">Service</a></li>
            <li class="mt-sm-3"><a href="#publication" class="scroll">Publication</a></li>
            <li class="mt-sm-3"><a href="#project" class="scroll">Project</a></li>
            <li class="mt-sm-3"><a href="#experience" class="scroll">Experience</a></li>
            <!-- <li class="mt-sm-3"><a href="#skill" class="scroll">Skills</a></li> -->
          </ul>
        </nav>
      </div>
    </header>
  </div>
  <div class="main" id="about">
    <div class="banner-text-w3ls">
      <div>
        <div class="mx-auto text-left">
          <h1><strong>Xin Chen - 陈 欣</strong></h1>
          <h5>Research Scientist, &ensp;Ph.D. of Computer Vision/Graphics</h5>
          <h6>chenxin2@shanghaitech.edu.cn</h6><br>
          <h5><a href="https://www.tencent.com/"><b>Tencent - 腾讯</b></a></h5>
          <h5><a href="http://english.ucas.ac.cn/"><b>University of Chinese Academy of Sciences - 中国科学院大学 </b></a></h5>
          <h5><a href="http://www.shanghaitech.edu.cn/"><b>ShanghaiTech University - 上海科技大学 </b></a> - <a
              href="http://sist.shanghaitech.edu.cn/">SIST</a> - <a href="http://vic.shanghaitech.edu.cn/">VDI</a></h5>
          <!-- <h6>&emsp;<a href="http://sist.shanghaitech.edu.cn/">School of Information Science and Technology</a></h6> -->
          <!-- <br> -->
          <!-- <h5><a href="https://www.dgene.com/cn/">DGene</a> </h5> -->
          <!-- <p>. </p> -->
          <!-- <p class="banp mt-5"> </p> -->
          <br>
          I'm a Researcher Scientist at <a href="https://www.tencent.com/">Tencent</a> (腾讯大咖计划), working with <a
            href="https://www.skicyyu.org/">Dr. Gang Yu</a> for <strong>Generative AI</strong>.
          <!-- and <a href="https://cshen.github.io/">Chunhua Shen</a> -->
          <!-- <br><br> -->
          I obtained my Ph.D. at ShanghaiTech University <a href="https://vic.shanghaitech.edu.cn/">Visual Intelligent Center (VIC)</a>, advised by <a
            href="http://www.yu-jingyi.com/">Prof. Jingyi Yu</a>. Before that, I was supervised by <a
            href="http://youyizheng.net/">Prof. Youyi Zheng</a> at <a
            href="http://english.ucas.ac.cn/">Chinese Academy of Sciences</a>, working on 3D modeling.
          <br><br>
          I currently have internship positions available with the goal of conducting cutting-edge research in artificial intelligence. If you are interested, please send me an email.
          <br><br>
          My research interests are <strong>Computer Vision</strong> for <strong>Graphics</strong>, especially:
          </p>
          <li>3D Generation,&nbsp;&nbsp;Avatar Generation,&nbsp;&nbsp;Motion Synthesis,&nbsp;&nbsp;View Synthesis</li>
          <li>Human Agents, Multi-modal Language Models, Embodied AI</li>
          <li>Human Shape and Motion Capture,&nbsp;&nbsp;Human Modeling and Dynamics,&nbsp;&nbsp;Image-based Reconstruction</li>
          <!-- <li>Geometric deep learning</li>        -->
          <!--a class="btn btn-primary mt-lg-5 mt-3 agile-link-bnr" href="#experience" role="button">Learn More</a-->
          <br>
          <font color="#f33" size="5"><b>News</b></font>
          </p>
          <!-- <li>[2022.11]&ensp; Submit four4 Papers.</li> -->
          <li>[2023.12]&ensp; We introduce <a href="https://github.com/OpenTexture/Paint3D">
            <font color="#007BFF"><b>Paint3D</b></font></a>, a lighting-less texture diffusion model.</li>
          <li>[2023.12]&ensp; We introduce <a href="https://github.com/mnotgod96/AppAgent">
              <font color="#007BFF"><b>AppAgent</b></font></a>, a multimodal agent for smartphone apps.</li>
          <li>[2023.11]&ensp; We introduce <a href="https://github.com/OpenShapeLab/ShapeGPT">
            <font color="#007BFF"><b>ShapeGPT</b></font></a>, a multimodal LLM for 3D shape generation.            
          <li>[2023.11]&ensp; We introduce <a href="https://github.com/Open3DA/LL3DA">
            <font color="#007BFF"><b>LL3DA</b></font></a> and <a href="https://github.com/OpenM3D/M3DBench"> <font color="#007BFF"><b>M3DBench</b></font></a>, a multimodal-language 3D assistant and benchmark</li>

          <li>[2023.09]&ensp; 3 paper accepted at <a href="https://nips.cc/">
            <font color="#f33"><b>NeurIPS 2023</b></font>
            </a></li>
          <li>[2023.07]&ensp; 1 paper accepted at <a href="https://iccv2023.thecvf.com/">
              <font color="#f33"><b>ICCV 2023</b></font>
            </a></li>
          <li>[2023.02]&ensp; 2 papers accepted at <a href="https://cvpr2023.thecvf.com/">
              <font color="#f33"><b>CVPR 2023</b></font>
            </a></li>
          <li>[2022.02]&ensp; Join Tencent as a Research Scientist</li>
          <li>[2022.01]&ensp; Receive my Ph.D. degree</li>
          <li>[2021.10]&ensp; TightCap will be presented at ACM <a href="https://s2022.siggraph.org/">
              <font color="#f33"><b>SIGGRAPH 2022</b></font>
            </a></li>
          <!-- <li>[2021.07]&ensp; TightCap is accepted by TOG. Dataset is released.</li>
          <li>[2021.06]&ensp; SportsCap is accepted by IJCV. Dataset is released.</li> -->
        </div>
      </div>
    </div>
    <section class="server" id="services">
      <h3 class="w3_head mb-1">Services</h3>
      <!-- <ul class="fa-ul mb-0"> -->
      <!-- <li> Talks:&ensp; 20230723 - VRVC, ShanghaiTech University</li> -->
      <!-- <li> Talks:&ensp; 20230702 - Embedded Vision Lab, Fudan University</li> -->
      <li> Talks:&ensp; <a href="https://juejin.cn/live/dmxyaigc006">20230701 - XDC 2023</a></li>
      <li> Conference/Journal Reviewer:&ensp; ICCV, CVPR, AAAI / TPAMI, TIP, IJCV, TMM, JVCI </li>
      <!-- <li> Journal Reviewer:&ensp; </li> -->
      <!-- </ul> -->
    </section>
    <section class="publication" id="publication">
      <div class="container-fluid py-lg-1">
        <h3 class="w3_head mb-5">Selected Publications <font size="5"><a href="https://scholar.google.com/citations?user=7qeAJZ4AAAAJ&hl=en">(Complete list…)</a>
          </font>
        </h3>
        <!-- <h3 class="w3_head mb-5">Selected Publications (<a href="./Publication.html">Complete List…</a>)</h3> -->
        <div class="col-lg-12">
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <video src="files/Paper/Paint3D/teaser.mp4" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
            </div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4><strong>Paint3D</strong>: Paint Anything 3D with Lighting-less Texture Diffusion Models<br>
              </h4>
              <p>Xianfang Zeng*, <strong>Xin Chen*</strong>, Zhongqi Qi*, <a
                  href="https://scholar.google.com/citations?user=A6K6bkoAAAAJ&hl=en">Wen Liu</a>, Zibo Zhao, Zhibin Wang, Bin Fu, <a
                  href="https://scholar.google.com/citations?user=qYcgBbEAAAAJ&hl=en">Yong Liu</a>, <a href="https://www.skicyyu.org/">Gang Yu</a> <br>
              </p>
              <p>
                <font color="#000000"><b>Arxiv 2023</b></font>
              </p>
              <!-- <p>(<strong>SIGGRAPH 2022</strong>) ACM Transactions on Graphics, TOG Journal Track</p> -->
              <!-- <p class="text-left">We present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan. </p> -->
              <p>[<a href="https://paint3d.github.io/">Project</a>]
                [<a href="https://arxiv.org/abs/2312.13913" target=_blank>Paper</a>]
                [<a href="https://github.com/OpenTexture/Paint3D" target=_blank>Code</a>] <a
                  href="https://github.com/OpenTexture/Paint3D" target=_blank><img alt="GitHub stars"
                    style="vertical-align: middle"
                    src="https://img.shields.io/github/stars/OpenTexture/Paint3D?style=social"></a>
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <video src="files/Paper/AppAgent/teaser.mp4" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
            </div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4><strong>AppAgent</strong>: Human Motion as a Foreign Language<br>
              </h4>
              <p><a
                href="https://icoz69.github.io/">Chi Zhang*</a>, <a
                href="https://github.com/yz93">Zhao Yang*</a>, <a
                href="https://www.linkedin.com/in/jiaxuan-liu-9051b7105/">Jiaxuan Liu*</a>, <a
                href="https://tingxueronghua.github.io/">Yuchen Han</a>, <strong>Xin Chen</strong>, Zebiao Huang, Bin Fu, <a
                  href="http://www.yu-jingyi.com/">Jingyi Yu</a>, <a href="https://www.skicyyu.org/">Gang Yu</a><br>
              </p>
              <p>
                <font color="#000000"><b>Arxiv 2023</b></font>
              </p>
              <!-- <p>(<strong>SIGGRAPH 2022</strong>) ACM Transactions on Graphics, TOG Journal Track</p> -->
              <!-- <p class="text-left">We present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan. </p> -->
              <p>[<a href="https://appagent-official.github.io/">Project</a>]
                [<a href="https://arxiv.org/abs/2312.13771" target=_blank>Paper</a>]
                [<a href="https://github.com/mnotgod96/AppAgent" target=_blank>Code</a>] <a
                  href="https://github.com/mnotgod96/AppAgent" target=_blank><img alt="GitHub stars"
                    style="vertical-align: middle"
                    src="https://img.shields.io/github/stars/mnotgod96/AppAgent?style=social"></a>
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <!--
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <video src="files/Paper/ShapeGPT/teaser.mp4" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
            </div>
            <div class="col-md-9 col-12 paper_content">
              <h4><strong>ShapeGPT</strong>: 3D Shape Generation with a Unified Multi-modal Language Model<br>
              </h4>
              <p>Fukun Ying*, <strong>Xin Chen†</strong>, <a
                  href="https://icoz69.github.io/">Chi Zhang</a>, <a
                  href="https://www.biaojiang.tech/">Biao Jiang</a>, Zibo Zhao, <a href="https://scholar.google.com.hk/citations?user=gsLd2ccAAAAJ&hl=zh-CN">Jiayuan Fan</a>, <a href="https://www.skicyyu.org/">Gang Yu</a>, Taihao Li, <a
                  href="https://faculty.fudan.edu.cn/chentao1/zh_CN/">Tao Chen‡</a> <br>
              </p>
              <p>
                <font color="#000000"><b>Arxiv 2023</b></font>
              </p>
              <p>[<a href="https://shapegpt.github.io/">Project</a>]
                [<a href="https://arxiv.org/abs/2311.17618" target=_blank>Paper</a>]
                [<a href="https://github.com/OpenShapeLab/ShapeGPT" target=_blank>Code</a>] <a
                  href="https://github.com/OpenShapeLab/ShapeGPT" target=_blank><img alt="GitHub stars"
                    style="vertical-align: middle"
                    src="https://img.shields.io/github/stars/OpenShapeLab/ShapeGPT?style=social"></a>
              </p>
            </div>
          -->
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <video src="files/Paper/NIPS23_MotionGPT/teaser.mp4" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
            </div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4><strong>MotionGPT</strong>: Human Motion as a Foreign Language<br>
              </h4>
              <p>Biao Jiang*, <strong>Xin Chen*</strong>, <a
                  href="https://scholar.google.com/citations?user=A6K6bkoAAAAJ&hl=en">Wen Liu</a>, <a
                  href="http://www.yu-jingyi.com/">Jingyi Yu</a>, <a href="https://www.skicyyu.org/">Gang Yu</a>, <a
                  href="https://faculty.fudan.edu.cn/chentao1/zh_CN/">Tao Chen</a> <br>
              </p>
              <p>
                <font color="#000000"><b>NeurIPS 2023</b></font>
              </p>
              <!-- <p>(<strong>SIGGRAPH 2022</strong>) ACM Transactions on Graphics, TOG Journal Track</p> -->
              <!-- <p class="text-left">We present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan. </p> -->
              <p>[<a href="https://motion-gpt.github.io/">Project</a>]
                [<a href="https://arxiv.org/abs/2306.14795" target=_blank>Paper</a>]
                [<a href="https://github.com/OpenMotionLab/MotionGPT" target=_blank>Code</a>] <a
                  href="https://github.com/OpenMotionLab/MotionGPT" target=_blank><img alt="GitHub stars"
                    style="vertical-align: middle"
                    src="https://img.shields.io/github/stars/OpenMotionLab/MotionGPT?style=social"></a>
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <video src="files/Paper/NIPS23_Michelangelo/teaser.mp4" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
            </div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4><strong>Michelangelo</strong>: Conditional 3D Shape Generation based on Shape-Image-Text Aligned
                Latent Representation<br>
              </h4>
              <p>Zibo Zhao*, <a href="https://scholar.google.com/citations?user=A6K6bkoAAAAJ&hl=en">Wen Liu</a>*,
                <strong>Xin Chen</strong>, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, <a
                  href="https://faculty.fudan.edu.cn/chentao1/zh_CN/">Tao Chen</a>,<a
                  href="https://www.skicyyu.org/">Gang Yu</a>, <a
                  href="https://scholar.google.com.sg/citations?user=fe-1v0MAAAAJ&hl=en">Shenghua Gao</a>
              </p>
              <p>
                <font color="#000000"><b>NeurIPS 2023</b></font>
              </p>
              <!-- <p>(<strong>SIGGRAPH 2022</strong>) ACM Transactions on Graphics, TOG Journal Track</p> -->
              <!-- <p class="text-left">We present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan. </p> -->
              <p>[<a href="https://neuralcarver.github.io/michelangelo/">Project</a>]
                [<a href="https://arxiv.org/abs/2306.17115" target=_blank>Paper</a>]
                [<a href="https://github.com/NeuralCarver/Michelangelo" target=_blank>Code</a>]<a
                  href="https://github.com/NeuralCarver/Michelangelo" target=_blank><img alt="GitHub stars"
                    style="vertical-align: middle"
                    src="https://img.shields.io/github/stars/NeuralCarver/Michelangelo?style=social"></a>
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <video src="files/Paper/ICCV23_OMMO/teaser.mp4" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
            </div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4>A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene
                Reconstruction<br>
              </h4>
              <p>Chongshan Lu, Fukun Yin, <strong>Xin Chen</strong>, <a
                  href="https://faculty.fudan.edu.cn/chentao1/zh_CN/">Tao Chen</a>, <a
                  href="https://www.skicyyu.org/">Gang Yu</a>, Jiayuan Fan
              </p>
              <p>
                <font color="#000000"><b>ICCV 2023</b></font>
              </p>
              <!-- <p>(<strong>SIGGRAPH 2022</strong>) ACM Transactions on Graphics, TOG Journal Track</p> -->
              <!-- <p class="text-left">We present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan. </p> -->
              <p>[<a href="https://ommo.luchongshan.com/">Project</a>]
                [<a href="https://arxiv.org/abs/2301.06782" target=_blank>Paper</a>]
                [<a href="https://github.com/luchongshan/OMMO" target=_blank>Code</a>]<a
                  href="https://github.com/luchongshan/OMMO" target=_blank><img alt="GitHub stars"
                    style="vertical-align: middle"
                    src="https://img.shields.io/github/stars/luchongshan/OMMO?style=social"></a>
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <img src="files/Paper/CVPR23_MLD/teaser.png" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
            </div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4>Executing your Commands via Motion Diffusion in Latent Space<br>
              </h4>
              <p><strong>Xin Chen*</strong>, Biao Jiang*, <a
                  href="https://scholar.google.com/citations?user=A6K6bkoAAAAJ&hl=en">Wen Liu</a>, <a
                  href="https://speedinghzl.github.io/">Zilong Huang</a>, Bin Fu, <a
                  href="https://faculty.fudan.edu.cn/chentao1/zh_CN/">Tao Chen</a>, <a
                  href="http://www.yu-jingyi.com/">Jingyi Yu</a>, <a href="https://www.skicyyu.org/">Gang Yu</a><br>
              </p>
              <p>
                <font color="#000000"><b>CVPR 2023</b></font>
              </p>
              <!-- <p>(<strong>SIGGRAPH 2022</strong>) ACM Transactions on Graphics, TOG Journal Track</p> -->
              <!-- <p class="text-left">We present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan. </p> -->
              <p>[<a href="https://chenxin.tech/mld">Project</a>]
                [<a href="https://arxiv.org/abs/2212.04048" target=_blank>Paper</a>]
                [<a href="https://github.com/ChenFengYe/motion-latent-diffusion" target=_blank>Code</a>]<a
                  href="https://github.com/ChenFengYe/motion-latent-diffusion" target=_blank><img alt="GitHub stars"
                    style="vertical-align: middle"
                    src="https://img.shields.io/github/stars/ChenFengYe/motion-latent-diffusion?style=social"></a>
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <img src="files/Paper/CVPR23_DETR/teaser.png" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
            </div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4>End-to-End 3D Dense Captioning with Vote2Cap-DETR<br>
              </h4>
              <p>Sijin Chen, <a href="https://hongyuanzhu.github.io/">Hongyuan Zhu</a>, <strong>Xin Chen</strong>,
                Yinjie
                Lei, <a href="https://faculty.fudan.edu.cn/chentao1/zh_CN/">Tao Chen</a>, <a
                  href="https://www.skicyyu.org/">Gang Yu</a><br>
              </p>
              <p>
                <font color="#000000"><b>CVPR 2023</b></font>
              </p>
              <!-- <p>(<strong>SIGGRAPH 2022</strong>) ACM Transactions on Graphics, TOG Journal Track</p> -->
              <!-- <p class="text-left">We present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan. </p> -->
              <p>[<a href="https://arxiv.org/abs/2301.02508" target=_blank>Paper</a>]
                [<a href="https://github.com/ch3cook-fdu/Vote2Cap-DETR" target=_blank>Code</a>]
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <video
                src="files/Paper/TOG2021_TightCap/project_page_TightCap/data/video_teaser.mp4" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
            </div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4><strong>TightCap</strong>: 3D Human Shape Capture with Clothing Tightness Field<br>
              </h4>
              <p><strong>Xin Chen</strong>, Anqi Pang, <a
                  href="https://scholar.google.com/citations?user=fRjxdPgAAAAJ&hl=en">Wei Yang</a>, Peihao Wang, <a
                  href="https://www.xu-lan.com/">Lan Xu</a>, <a href="http://www.yu-jingyi.com/">Jingyi Yu</a><br>
              </p>
              <p>
                <font color="#000000"><b>SIGGRAPH 2022</b></font> TOG Journal Track
              </p>
              <!-- <p>(<strong>SIGGRAPH 2022</strong>) ACM Transactions on Graphics, TOG Journal Track</p> -->
              <!-- <p class="text-left">We present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan. </p> -->
              <p>[<a href="./TightCap">Project</a>] [<a href="https://github.com/ChenFengYe/TightCap">Code</a>] [<a
                  href="https://arxiv.org/abs/1904.02601">Arxiv</a>]
                [<a href="files/Paper\TOG2021_TightCap\project_page_TightCap\data\TightCap.pdf" target=_blank>Paper</a>]
                [<a href="files/Paper\TOG2021_TightCap\project_page_TightCap\data\video.mp4" target=_blank>Video</a>]
                [<a href="files\Paper\TOG2021_TightCap\project_page_TightCap\data\bibtex.html"
                  target=_blank>BibTex</a>]<a href="https://github.com/ChenFengYe/TightCap" target=_blank><img
                    alt="GitHub stars" style="vertical-align: middle"
                    src="https://img.shields.io/github/stars/ChenFengYe/TightCap?style=social"></a>
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <video
                src="files/Paper/IJCV2020_Sport/project_page_SportsCap/data/video_teaser.mp4" width="90%" playsinline=""
                autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid"></div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4><strong>SportsCap</strong>: Monocular 3D Human Motion Capture and Fine-grained Understanding in
                Challenging Sports Videos<br>
              </h4>
              <p><strong>Xin Chen</strong>, Anqi Pang, <a
                  href="https://scholar.google.com/citations?user=fRjxdPgAAAAJ&hl=en">Wei Yang</a>, <a
                  href="http://yuexinma.me/aboutme.html">Yuexin Ma</a>, <a href="http://xu-lan.com/">Lan Xu</a>, <a
                  href="http://www.yu-jingyi.com/">Jingyi Yu</a><br>
              </p>
              <p>
                <font color="#000000"><b>IJCV 2021</b></font>
              </p>
              <!-- <p>(<strong>IJCV 2021</strong>) International Journal of Computer Vision</p> -->
              <!-- <p class="text-left">We propose SportsCap – the first approach for simultaneously capturing 3D human motions and understanding fine-grained actions from monocular challenging sports video input. </p> -->
              <p>[<a href="./SportsCap.html">Project</a>] [<a href="https://github.com/ChenFengYe/SportsCap">Code</a>]
                [<a href="https://arxiv.org/abs/2104.11452">Arxiv</a>]
                [<a href="files\Paper\IJCV2020_Sport\project_page_SportsCap\data\SportsCap.pdf">Paper</a>] [<a
                  href="files\Paper\IJCV2020_Sport\project_page_SportsCap\data\video.mp4" target=_blank>Video</a>] [<a
                  href="files\Paper\IJCV2020_Sport\project_page_SportsCap\data\bibtex.html" target=_blank>BibTex</a>]<a
                  href="https://github.com/ChenFengYe/SportsCap" target=_blank><img alt="GitHub stars"
                    style="vertical-align: middle"
                    src="https://img.shields.io/github/stars/ChenFengYe/SportsCap?style=social"></a>
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <video src="files/Paper/CVPR2021_ChallenCap/video_teaser.mp4"
                width="90%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image"
                class="img-fluid" /></div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4><strong>ChallenCap</strong>: Monocular 3D Capture of Challenging Human Performances using Multi-Modal
                References<br>
              </h4>
              <p><a href="https://hynann.github.io/">Yannao He</a>, Anqi Pang, <strong>Xin Chen</strong>, Han Liang, <a
                  href="http://yuexinma.me/aboutme.html">Yuexin Ma</a>, <a href="http://xu-lan.com/">Lan Xu</a><br>
              </p>
              <p>
                <font color="#000000"><b>CVPR 2021 Oral</b></font>
              </p>
              <!-- <p>(<strong>CVPR 2021 Oral</strong>) Conference on Computer Vision and Pattern Recognition</p> -->
              <!-- <p class="text-left">We propose SportsCap – the first approach for simultaneously capturing 3D human motions and understanding fine-grained actions from monocular challenging sports video input. </p> -->
              <p>[Project] [<a href="https://arxiv.org/abs/2103.06747">Arxiv</a>] [<a
                  href="https://openaccess.thecvf.com/content/CVPR2021/papers/He_ChallenCap_Monocular_3D_Capture_of_Challenging_Human_Performances_Using_Multi-Modal_CVPR_2021_paper.pdf"
                  target=_blank>Paper</a>] [<a href="https://www.youtube.com/watch?v=ctF0xUMoN2E"
                  target=_blank>Video</a>] [<a href="files\Paper\CVPR2021_ChallenCap\bibtex.html"
                  target=_blank>BibTex</a>] </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img">
              <img src="files/Paper/AllPublications/AAAI2022.png" width="90%" playsinline="" autoplay="autoplay"
                loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid" />
            </div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4>Anisotropic Fourier Features for Neural Image-Based Rendering and Relighting</h4>
              <p>Huangjie Yu, Anpei Chen, <strong>Xin Chen</strong>, <a href="https://www.xu-lan.com/">Lan Xu</a>, Ziyu
                Shao, <a href="http://www.yu-jingyi.com/">Jingyi Yu</a></p>
              <p>
                <font color="#000000"><b>AAAI 2022 Oral</b></font>
              </p>
              <!-- <p>(<strong>AAAI 2022 Oral</strong>) the Association for the Advance of Artificial Intelligence</p> -->
              <!-- <p class="text-left">We present a fully automatic framework for extracting editable 3D objects directly from a single photograph. </p> -->
              <p>[<a href="https://www.aaai.org/AAAI22Papers/AAAI-2220.YuH.pdf">Paper</a>] [BibTex]
              </p>
            </div>
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <video src="files/Paper/IJCAI2021_FewShot/video_teaser.mp4"
                width="90%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image"
                class="img-fluid" /></div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4>Few-shot Neural Human Performance Rendering from Sparse RGBD Videos<br>
              </h4>
              <p>Anqi Pang*, <strong>Xin Chen*</strong>, Haimin Luo, <a href="https://wuminye.com/">Mingye Wu</a>, <a
                  href="http://www.yu-jingyi.com/">Jingyi Yu</a>, <a href="http://xu-lan.com/">Lan Xu</a><br>
              </p>
              <p>
                <font color="#000000"><b>IJCAI 2021</b></font>
              </p>
              <!-- <p>(<strong>IJCAI 2021</strong>) International Joint Conference on Artificial Intelligence</p> -->
              <!-- <p class="text-left">We propose SportsCap – the first approach for simultaneously capturing 3D human motions and understanding fine-grained actions from monocular challenging sports video input. </p> -->
              <p>[Project] [<a href="https://arxiv.org/abs/2107.06505">Arxiv</a>] [<a
                  href="https://arxiv.org/abs/2107.06505" target=_blank>Paper</a>] [<a
                  href="files\Paper\IJCAI2021_FewShot\video_teaser.mp4" target=_blank>Video</a>] [<a
                  href="files\Paper\IJCAI2021_FewShot\bibtex.html" target=_blank>BibTex</a>]
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <video src="files\Paper\MM2021_HOIFVV\video_teaser.mp4" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image"
                class="img-fluid" /></div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4>Neural Free-Viewpoint Performance Rendering under ComplexHuman-object Interactions<br>
              </h4>
              <p>Guoxing Sun, <strong>Xin Chen</strong>, Yizhang Chen, Anqi Pang, Pei Lin, Yuheng Jiang, <a
                  href="http://xu-lan.com/">Lan Xu</a>, <a
                  href="http://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a>, <a
                  href="http://www.yu-jingyi.com/">Jingyi Yu</a><br>
              </p>
              <p>
                <font color="#000000"><b>ACMMM 2021</b></font>
              </p>
              <!-- <p>(<strong>ACMMM 2021</strong>) ACM Multimedia</p> -->
              <!-- <p class="text-left">We propose SportsCap – the first approach for simultaneously capturing 3D human motions and understanding fine-grained actions from monocular challenging sports video input. </p> -->
              <p>[Project] [<a href="https://arxiv.org/abs/2108.00362">Arxiv</a>] [<a
                  href="https://arxiv.org/abs/2108.00362" target=_blank>Paper</a>] [<a
                  href="files\Paper\MM2021_HOIFVV\video_teaser.mp4" target=_blank>Video</a>] [<a
                  href="files\Paper\MM2021_HOIFVV\bibtex.html" target=_blank>BibTex</a>]
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-3 col-12 paper_img"> <video src="files/Paper/CVPR2018_Face/3d_face.mp4" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
            </div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4>Sparse Photometric 3D Face Reconstruction Guided by Morphable Models</h4>
              <p> Xuan Cao, Zhang Chen, Anpei Chen, <strong>Xin Chen</strong>, Shiying Li, <a
                  href="http://www.yu-jingyi.com/">Jingyi Yu</a></p>
              <p>
                <font color="#000000"><b>CVPR 2018</b></font>
              </p>
              <!-- <p>(<strong>CVPR 2018</strong>) IEEE Conference on Computer Vision and Pattern Recognition</p> -->
              <!-- <p class="text-left">We present a novel 3D face reconstruction technique that leverages sparse photometric stereo (PS) and the latest advances on face registration modeling from a single image. </p> -->
              <p>[<a href="files/Paper/CVPR2018_Face/Sparse_Photometric_3D_CVPR_2018_paper.pdf">Paper</a>] [<a
                  href="files/Paper/CVPR2018_Face/Sparse_Photometric_3D_CVPR_2018_video.mp4">Video</a>] [<a
                  href="files/Paper/CVPR2018_Face/Sparse_Photometric_3D_CVPR_2018_supp.pdf">Supp</a>] [<a
                  href="files/Paper/CVPR2018_Face/cao2018sparse.html" target=_blank>BibTex</a>] </p>
            </div>
          </div>
          <div class="row paper_box">
            <!-- <div class="col-md-3 col-12 paper_img"> <img src="files/Paper/TVCG2018_AutoSweep/AutoSweep.jpg" alt="Popup Image" class="img-fluid" /></div> -->
            <div class="col-md-3 col-12 paper_img"> <video
                src="files/Paper/TVCG2018_AutoSweep/project_page_AutoSweep/data/video.mp4" width="90%" playsinline=""
                autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid"></div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper_content">
              <h4><strong>AutoSweep</strong>: Recovering 3D Editable Objects&nbsp;from a Single Photograph</h4>
              <p><strong>Xin Chen</strong>, <a href="http://liyuwei.cc/">Yuwei Li</a>, <a href="http://luoxi.tech/">Xi
                  Luo</a>, <a href="http://tianjiashao.com/">Tianjia Shao</a>, <a
                  href="http://www.yu-jingyi.com/">Jingyi Yu</a>, <a href="http://kunzhou.net/">Kun Zhou</a>, <a
                  href="http://youyizheng.net/">Youyi Zheng </a></p>
              <p>
                <font color="#000000"><b>TVCG 2018</b></font>
              </p>
              <!-- <p>(<strong>TVCG 2018</strong>) IEEE Transactions on Visualization and Computer Graphics</p> -->
              <!-- <p class="text-left">We present a fully automatic framework for extracting editable 3D objects directly from a single photograph. </p> -->
              <p>[<a href="./AutoSweep.html">Project</a>] [<a href="https://github.com/ChenFengYe/AutoSweep">Code</a>]
                [<a href="https://arxiv.org/abs/2005.13312">Arxiv</a>]
                [<a href="files/Paper/TVCG2018_AutoSweep/project_page_AutoSweep/data/AutoSweep.pdf">Paper</a>] [<a
                  href="files/Paper/TVCG2018_AutoSweep/project_page_AutoSweep/data/video.mp4">Video</a>] [<a
                  href="files/Paper/TVCG2018_AutoSweep/project_page_AutoSweep/data/bibtex.html"
                  target=_blank>BibTex</a>]
              </p>
              <br>
              <p>
              <h4><a href="./Publication.html">See all publications ...</a></h4>
              </p>
            </div>
            <div class="row paper_box">
              <!-- <div class="col-md-3 col-12 paper_img"> <img src="files/Paper/TVCG2018_AutoSweep/AutoSweep.jpg" alt="Popup Image" class="img-fluid" /></div> -->
              <div class="col-md-3 col-12 paper_img"></div>
              <!-- .Icon ends here -->
              <div class="col-md-9 col-12 paper_content">
                </p>
              </div>
              <!-- .Service-content ends here -->
            </div>
          </div>
        </div>
    </section>
    <!-- project -->
    <section class="wedo" id="project">
      <div class="project">
        <h3 class="w3_head mb-1">Projects<font size="5"> <a href="./index.html">(More projects…)</a>
          </font>
        </h3>
        <div class="row service_w3top mt-5">
          <div class="col-xl-12">
            <!-- Masonry Grid -->
            <div class="masonry-grid">
                <div class="masonry-grid-item col-xs-4 col-sm-4 col-lg-4 mr-0">
                  <!-- Work -->
                  <div class="work">
                    <div class="work-overlay">
                      <!-- <img class="full-width img-responsive" src="project/img/image1.jpg" alt="Portfolio Image"> -->
                      <video src="files/Paper/NIPS23_MotionGPT/teaser.mp4" playsinline="" autoplay="autoplay"
                        loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
                        <!-- <img class="full-width img-responsive" src="project/img/image1.jpg" alt="Portfolio Image"> -->
                    </div>
                    <div class="work-content">
                      <h4 class="color-white margin-b-5">MotionGPT:Motion-Language Generations</h4>
                      <p class="color-white margin-b-0 fs-6"> A unified and user-friendly motion-language model to learn the semantic coupling of two modalities and generate high-quality motions and text descriptions on multiple motion tasks.</p>
                    </div>
                  </div>
                </div>
                <div class="masonry-grid-item col-xs-2 col-sm-2 col-lg-2 ml-0 mr-2">
                  <!-- Work -->
                  <div class="work">
                    <div class="work-overlay">
                      <img class="full-width img-responsive" src="project/img/image14.jpg" alt="Portfolio Image">
                    </div>
                    <div class="work-content">
                      <p class="color-white margin-b-5">Motion Latent Diffusion Models</p>
                      <!-- <p class="color-white margin-b-0 fs-6">My team develop a purpose-built capture stage, and proprietary reconstruction software. We make it possible to record performances and convert them into 3D sequences viewable from any angle.</p> -->
                    </div>
                  </div>
                </div>


                <!-- raw 01 - col 02 -->
                <div class="masonry-grid-item col-xs-4 col-sm-4 col-lg-4 mr-0">
                  <!-- Work -->
                  <div class="work">
                    <div class="work-overlay">
                      <video src="project/img/media3_2.mp4" playsinline="" autoplay="autoplay"
                      loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
                    </div>
                    <div class="work-content">
                      <h4 class="color-white margin-b-5">Human-Object Motion Capture</h4>
                      <p class="color-white margin-b-0 fs-6"> Using 13 infrared cameras for complex motion capture. We collected a complex motion dataset to assist challenging monocular motion capture for sports video.</p>
                    </div>
                  </div>
                </div>
                <div class="masonry-grid-item col-xs-2 col-sm-2 col-lg-2">
                  <!-- Work -->
                  <div class="work">
                    <div class="work-overlay">
                      <img class="full-width img-responsive" src="project/img/image12_2.png" alt="Portfolio Image">
                    </div>
                    <div class="work-content">
                      <h4 class="color-white margin-b-5">Light Stage</h4>
                      <p class="color-white fs-6">Multi-view light stage system for MoCap and 3D Reconstruction.</p>
                    </div>
                  </div>
                </div>
              </div>

            <!-- Masonry Grid -->
            <div class="masonry-grid">
              <div class="masonry-grid-item col-xs-2 col-sm-2 col-lg-2">
                <!-- Work -->
                <div class="work">
                  <div class="work-overlay">
                    <video src="project/img/media7.mp4" playsinline="" autoplay="autoplay"
                      loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
                  </div>
                  <div class="work-content">
                    <h5 class="color-white margin-b-5">Dynamic 3D Mesh Player</h5>
                    <p class="color-white margin-b-0 fs-6">Stand-alone development work for free-view browsing on 4D scans, which supports 3D mesh rendering, free-angle viewpoint change, Poisson Surface Reconstruction.</p>
                  </div>
                </div>
              </div>
              <div class="masonry-grid-item col-xs-2 col-sm-2 col-lg-2 ml-0">
                <!-- Work -->
                <div class="work">
                  <div class="work-overlay">
                    <video src="project/img/media2_2.mp4" playsinline="" autoplay="autoplay"
                      loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
                  </div>
                  <div class="work-content">
                    <p class="color-white margin-b-0 fs-6">I am as a performer for boxing.</p>
                  </div>
                </div>
              </div>

              <div class="masonry-grid-item col-xs-2 col-sm-2 col-lg-2 ml-0 mr-2">
                <!-- Work -->
                <div class="work">
                  <div class="work-overlay">
                    <img class="full-width img-responsive" src="project/img/image1.jpg" alt="Portfolio Image">
                  </div>
                  <div class="work-content">
                    <h5 class="color-white margin-b-5">Multi-view Dome System</h5>
                    <p class="color-white margin-b-0 fs-6">Using more than 72 cameras for dynamic human reconstruction.</p>
                  </div>
                </div>
              </div>

              <!-- raw 01 - col 02 -->
              <div class="masonry-grid-item col-xs-2 col-sm-2 col-lg-2">
                <!-- Work -->
                <div class="work">
                  <div class="work-overlay">
                    <video src="project/img/media4_2.mp4" playsinline="" autoplay="autoplay"
                    loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
                  </div>
                  <div class="work-content">
                    <p class="color-white margin-b-0 fs-6">Poping result with free view-points.</p>
                  </div>
                </div>
              </div>
              <div class="masonry-grid-item col-xs-2 col-sm-2 col-lg-2">
                <!-- Work -->
                <div class="work">
                  <div class="work-overlay">
                    <video src="project/img/media5.mp4" playsinline="" autoplay="autoplay"
                    loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
                  </div>
                  <div class="work-content">
                    <p class="color-white margin-b-5">Avatar with high fidelity clothes and face.</p>
                  </div>
                </div>
              </div>
              <space>
              <div class="masonry-grid-item col-xs-2 col-sm-2 col-lg-2">
                <!-- Work -->
                <div class="work">
                  <div class="work-overlay">
                    <img class="full-width img-responsive" src="project/img/image8.jpeg" alt="Portfolio Image">
                  </div>
                  <div class="work-content">
                    <h5 class="color-white margin-b-5">Dome System for static objects</h5>
                    <p class="color-white margin-b-5">Using more than 80 cameras for multi-view stereo reconstruction. Lead the reconstruction project and aim to build a large 3D mesh sequences dataset.</p>
                    
                  </div>
                </div>
              </div>



            </div>
·
          </div>
        </div>
      </div>
    </section>
    <!-- experience -->
    <section class="wedo" id="experience">
      <div class="experience">
        <h3 class="w3_head mb-1">Experiences</h3>
        <div class="row service_w3top mt-5">
          <div class="col-xl-12">
            <div class="d-flex experience-box">
              <!-- <div class="popup"> <img src="images/g6.jpg" alt="Popup Image" class="img-fluid"></div> -->
              <div class="icon"> <span class="fa fa-briefcase"></span></div>
              <!-- <div class="icon"> <span class="fa fa-briefcase" style=" background-image: url('1.png'); background-repeat:no-repeat;display:block;align-items: center;" ></span> </div> -->
              <!-- .Icon ends here -->
              <div class="service-content">
                <div class="d-md-flex justify-content-between">
                  <h4><a href="https://open.youtu.qq.com/#/open">Tencent - 腾讯 </a></h4>
                </div>
                <div class="d-md-flex justify-content-between">
                  <h4> PCG QQ - Research Scientist</h4>
                  <h4> Feb. 2022 - Present</h4>
                </div>
                <div class="d-md-flex justify-content-between">
                  <h4> CSIG YouTu Lab -Research Scientist Intern</h4>
                  <h4>Nov. 2020 - Mar. 2021</h4>
                </div>
                <p>I am working at Tencent, Shanghai and received <strong>Tencent STAR Award</strong> 2023.</p>
                <p>I was a research scientist intern at <a href="https://open.youtu.qq.com/#/open">Tencent YouTu
                    Lab</a> in 2020 focuing on the 3D human body reconstruction.</p>
                <br>
              </div>
              <!-- .Service-content ends here -->
            </div>
            <!-- <div class="d-flex experience-box"> -->
            <!-- <div class="icon"><span class="fa fa-briefcase"></span> </div> -->
            <!-- .Icon ends here -->
            <!-- <div class="service-content">
                <div class="d-md-flex justify-content-between">
                  <h4><a href="https://www.dgene.com/cn/">DGene Digital Technology Inc. - 上海叠境数字</a></h4>
                  <h4> Jul. 2018 - Dec. 2019</h4>
                </div>
                <h4> R&amp;D Intern</h4>
                <p>I work as a part-time research and development intern at <a href="https://www.dgene.com/eng/">DGene
                    Digital Technology Inc. </a>and won <strong>the Best Outstanding Intern Award</strong> in 2018 for
                  leading the mobile virtual fitting project.</p>
                <br>
              </div> -->
            <!-- .Service-content ends here -->
            <!-- </div> -->
          </div>
        </div>
        <div class="row service_w3top mt-5">
          <div class="col-xl-12">
            <div class="d-flex experience-box">
              <div class="icon"> <span class="fa fa-graduation-cap"></span> </div>
              <!-- .Icon ends here -->
              <div class="service-content">
                <div class="d-md-flex justify-content-between">
                  <h4><a href="http://english.ucas.ac.cn/">University of Chinese Academy of Sciences - 中国科学院大学</a></h4>
                  <h4>Sep. 2018 - Feb. 2022 </h4>
                </div>
                <h4> Ph.D. Degree</h4>
                <p>I received my Ph.D. degree from <a href="http://english.ucas.ac.cn/">UCAS</a> under
                  the supervision of <a href="http://www.yu-jingyi.com/">Prof. Jingyi Yu,</a> working on <strong>human
                    performance capture</strong> including human reconstruction, clothing capture, and shape estimation.
                </p>
                <br>
              </div>
              <!-- .Service-content ends here -->
            </div>
            <div class="d-flex experience-box">
              <div class="icon"> <span class="fa fa-graduation-cap"></span> </div>
              <!-- .Icon ends here -->
              <div class="service-content">
                <div class="d-md-flex justify-content-between">
                  <h4><a href="http://www.shanghaitech.edu.cn/eng/">ShanghaiTech University - 上海科技大学</a></h4>
                  <h4>Sep. 2016 - Jul. 2018 </h4>
                </div>
                <h4> Master Student</h4>
                <p>I spent 2 years on computer graphics research, advised by <a href="http://youyizheng.net/">Prof.
                    Youyi
                    Zheng</a> and developed a fully automatic framework for recovering 3D editable objects from a single
                  photograph, published on <strong>TVCG 2018</strong> (Top journal in graphics).</p>
                <br>
              </div>
              <!-- .Service-content ends here -->
            </div>
            <div class="d-flex experience-box">
              <div class="icon"> <span class="fa fa-graduation-cap"></span> </div>
              <!-- .Icon ends here -->
              <div class="service-content">
                <div class="d-md-flex justify-content-between">
                  <h4><a href="http://english.qut.edu.cn/">Qingdao University of Technology</a></h4>
                  <h4>Sep. 2012 - Jul. 2016 </h4>
                </div>
                <h4> Bachelor </h4>
                <p>I received a B.S. degree in electronic information science and technology and won honorable awards
                  including: </p>
                <li><strong>National Encouragement Scholarship</strong>, 2016</li>
                <li><strong>Provincial Government Scholarship</strong>, 2015</li>
                <li><strong>Second Prize in China Mathematical Contest in Modeling</strong>, 2015</li>
                <li><strong>Province-level Merit Student</strong>, 2013</li>
              </div>
              <!-- .Service-content ends here -->
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="wedo" id="-Supervisors">
      <h3 class="w3_head mb-1"> Supervisors </h3>
      <div id="id4" style="clear:both"></div>
      <div class="col-md-5 col-12 people-content">
        <p class="banp mt-5"> <a href="http://www.yu-jingyi.com/">Prof. Jingyi Yu</a> - Supervisor </p>
        <li>IEEE Fellow</li>
        <li>Program chair of ICPR 2020 and CVPR 2021</li>
        <li>Executive Dean, SIST, ShanghaiTech University</li>
        <li>Founder, DGene Inc.</li>
        <li>Ph.D.'s degree, MIT, 2005</li>
      </div>
      <div class="col-md-5 col-12 people-content">
        <p class="banp mt-5"> <a href="http://youyizheng.net/">Prof. Youyi Zheng</a> - Former Supervisor </p>
        <li>Hundred Talents Program, ZheJiang Univeristy</li>
        <li>Researcher, State Key Lab of CAD&CG, ZheJiang Univeristy</li>
        <li>Ph.D.'s degree, Hong Kong University of Science and Technology, 2011</li>
      </div>
      <div id="id4" style="clear:both"></div>
    </section>

    <!-- //skill -->
    <!-- <section class="wedo" id="skill">
      <h3 class="w3_head mb-1">Skills </h3>
      <p class="banp mt-5"> Programming Languages</p>
      <ul class="fa-ul mb-0">
        <li> <i class="fa-li fa fa-check"></i> C++ (OpenGL, OpenCV, Qt, Eigen, PCL, CUDA and so on. )</li>
        <li> <i class="fa-li fa fa-check"></i> C#</li>
        <li> <i class="fa-li fa fa-check"></i> Python (Pytorch, Tensorflow, Mxnet)</li>
      </ul>
      <p class="banp mt-5"> Softwares</p>
      <ul class="fa-ul mb-0">
        <li> <i class="fa-li fa fa-check"></i> Visual Studio, Pycharm, Jupyter Notebook, Android Studio</li>
        <li> <i class="fa-li fa fa-check"></i> Matlab</li>
        <li> <i class="fa-li fa fa-check"></i> Unity3D, Blender</li>
        <li> <i class="fa-li fa fa-check"></i> Adobe Photoshop, Premiere</li>
      </ul>
      <p class="banp mt-5"> Others</p>
      <ul class="fa-ul mb-0">
        <li> <i class="fa-li fa fa-check"></i> Latex, Markdown</li>
        <li> <i class="fa-li fa fa-check"></i> Leap Motion, HTC Vive</li>
      </ul>
      <script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.?i=5ix9r8rqpnb&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>
    <script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=59qpe4r4c9p&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script>
    </section> -->
    <!-- //skill -->
    <!-- <section> -->
    <script type='text/javascript' id='clustrmaps'
      src='//cdn.clustrmaps.com/map_v2.js?cl=b5b5b5&w=300&t=tt&d=O8RXzIRuaS2aZrhlSVF9Mo3sC3cJziACrnPJbcG9mF4&co=ffffff&ct=000000&cmo=ff0000&cmn=00b6ff'></script>
</body>

</html>